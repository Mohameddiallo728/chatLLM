{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import Ollama\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "import os\n",
    "import pickle\n",
    "import asyncio\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model mistral\n",
    "model = Ollama(model=\"mistral_copy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"/home/mohamed/Documents/Mohamed/Mohamed_DIALLO_CV.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyMuPDFLoader(pdf_path)\n",
    "doc = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définir les séparateurs pour le découpage du texte\n",
    "separators = [\n",
    "        \"\\n\\n\",\n",
    "        \"\\n\",\n",
    "        \" \",\n",
    "        \".\",\n",
    "        \",\",\n",
    "        \"\\u200b\",  # Zero-width space\n",
    "        \"\\uff0c\",  # Fullwidth comma\n",
    "        \"\\u3001\",  # Ideographic comma\n",
    "        \"\\uff0e\",  # Fullwidth full stop\n",
    "        \"\\u3002\",  # Ideographic full stop\n",
    "        \"\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialiser le découpeur de texte avec les paramètres spécifiés\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=separators,\n",
    "    chunk_size=300, # Taille de chaque chunk en caractères\n",
    "    chunk_overlap=50, # Chevauchement entre les chunks consécutifs (réduit pour moins de chunks)\n",
    "    length_function=len, # Fonction pour calculer la longueur du texte\n",
    "    add_start_index=True, # Ajouter l'index de début à chaque chunk\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 2 documents into 13 chunks.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "chunks = text_splitter.split_documents(doc)\n",
    "print(f\"Split {len(doc)} documents into {len(chunks)} chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kalaban Coura ACI, Bamako, Rue 418 \n",
      "Tel : +223 62 09 21 57 \n",
      "Email : mohameddiallo728@gmail.com \n",
      "MOHAMED DIALLO \n",
      "Maîtrisant des outils tels que MySQL, PostgreSQL, je suis capable de façonner et d'optimiser des bases de données\n",
      "{'source': '/home/mohamed/Documents/Mohamed/Mohamed_DIALLO_CV.pdf', 'file_path': '/home/mohamed/Documents/Mohamed/Mohamed_DIALLO_CV.pdf', 'page': 0, 'total_pages': 2, 'format': 'PDF 1.7', 'title': '', 'author': 'Mohamed DIALLO', 'subject': '', 'keywords': '', 'creator': 'Microsoft® Word 2019', 'producer': 'Microsoft® Word 2019', 'creationDate': \"D:20240216080534+00'00'\", 'modDate': \"D:20240216080534+00'00'\", 'trapped': '', 'start_index': 0}\n"
     ]
    }
   ],
   "source": [
    "# Afficher un exemple de contenu de page et de métadonnées pour un chunk\n",
    "page = chunks[0]\n",
    "print(page.page_content)\n",
    "print(page.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chemin vers le répertoire pour enregistrer la base de données Chroma\n",
    "CHROMA_PATH = \"embdb\"\n",
    "EMB_CACHE_PATH = \"emb_cache.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-25 18:06:34.771390: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-07-25 18:06:34.958922: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-25 18:06:35.047772: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-25 18:06:35.074014: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-25 18:06:35.204122: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-25 18:06:37.450686: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# Récupérer la fonction d'embeddings à partir des ressources du code env\n",
    "emb_model = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "embeddings = HuggingFaceEmbeddings(model_name=emb_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les embeddings depuis le cache ou les générer si le cache n'existe pas\n",
    "if os.path.exists(EMB_CACHE_PATH):\n",
    "    with open(EMB_CACHE_PATH, 'rb') as f:\n",
    "        cached_embeddings = pickle.load(f)\n",
    "else:\n",
    "    # Générer les embeddings pour chaque chunk\n",
    "    cached_embeddings = embeddings.embed_documents([chunk.page_content for chunk in chunks])\n",
    "    with open(EMB_CACHE_PATH, 'wb') as f:\n",
    "        pickle.dump(cached_embeddings, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2c9a8cb5-f7ac-48b9-88fa-2fcf0956ba59',\n",
       " '0677d336-509f-4622-83a9-d01db584e50e',\n",
       " '4a32c887-c9e0-4404-8739-b0300d8eb227',\n",
       " '8b72bade-8244-406e-8b82-d62f42e2dc4f',\n",
       " '6f9c64f3-75d7-4895-a533-4282734fa2e6',\n",
       " '6bd22030-0a67-443d-a76d-0f52e909cb00',\n",
       " '682f2d65-8226-4ad3-afd3-cbab7bdacb9c',\n",
       " '4e58b653-344e-4a0b-9665-c5fa6c5c8145',\n",
       " '15d00a45-8c03-4606-921f-afdcda6cb134',\n",
       " '0ab7e633-c14b-4b3b-906f-22be4f2f7098',\n",
       " 'c6fdcd8c-358e-4e10-8a60-cff5df2d2b8d',\n",
       " '385db1b7-e4a6-4ffc-bdd6-45dc7655419d',\n",
       " '09800c90-ac29-4f8e-a847-f8750dc7df9a']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Créer une nouvelle base de données Chroma et ajouter les documents\n",
    "db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embeddings)\n",
    "db.add_documents(documents=chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enregistrement de 13 chunks dans embdb.\n"
     ]
    }
   ],
   "source": [
    "# Persister la base de données sur le disque\n",
    "db.persist()\n",
    "print(f\"Enregistrement de {len(chunks)} chunks dans {CHROMA_PATH}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compress prompts\n",
    "def compress_prompt(context, question):\n",
    "    # Keep only the most relevant parts of the context\n",
    "    compressed_context = context[:1000]  # Limiter à 1000 caractères par exemple\n",
    "    return f\"Question: {question}\\nContext: {compressed_context}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to handle query processing\n",
    "async def process_query(query_text):\n",
    "    # Retrieve context from DB using similarity search\n",
    "    results = db.similarity_search_with_relevance_scores(query_text, k=5)  # Récupérer plus de résultats pour le ranking\n",
    "    results.sort(key=lambda x: x[1], reverse=True)  # Tri par score de pertinence\n",
    "    top_results = results[:3]  # Prendre les 3 meilleurs résultats\n",
    "\n",
    "    context_text = \"\\n\\n - -\\n\\n\".join([doc.page_content for doc, _score in top_results])\n",
    "\n",
    "    # Compress the prompt\n",
    "    compressed_prompt = compress_prompt(context_text, query_text)\n",
    "\n",
    "    # Create prompt using compressed context and query text\n",
    "    prompt_template = ChatPromptTemplate.from_template(\"\"\"\n",
    "    Tu t'appelles Okka.\n",
    "    Tu es un assistant en intelligence artificielle conçu pour aider les utilisateurs en récupérant des informations pertinentes et en générant des réponses basées sur ces informations.\n",
    "    Ton objectif est de fournir des réponses claires et utiles.\n",
    "\n",
    "    {compressed_prompt}\n",
    "    \"\"\")\n",
    "\n",
    "    # Generate response based on the prompt\n",
    "    response_text = model.invoke(prompt_template.format(compressed_prompt=compressed_prompt))\n",
    "\n",
    "    # Get sources of the relevant documents\n",
    "    sources = [doc.metadata.get(\"source\", None) for doc, _score in top_results]\n",
    "\n",
    "    # Format and return the response\n",
    "    formatted_response = f\"Response: {response_text}\\nSources: {sources}\"\n",
    "    print(formatted_response)\n",
    "    return formatted_response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_text = \"Quelles competences Mohamed possède t'il ?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:  Mohamed possède des compétences en développement backend. Durant son dernier stage de 11 mois, il a effectué la rétro-ingénierie d'applications d'administration de gestion de courses, chauffeurs, clients et recouvrements. Il est expérimenté dans une gamme diversifiée de langages de programmation allant du PHP au Python en passant par le Java.\n",
      "Sources: ['/home/mohamed/Documents/Mohamed/Mohamed_DIALLO_CV.pdf', '/home/mohamed/Documents/Mohamed/Mohamed_DIALLO_CV.pdf', '/home/mohamed/Documents/Mohamed/Mohamed_DIALLO_CV.pdf']\n"
     ]
    }
   ],
   "source": [
    "# Retrieve context from DB using similarity search\n",
    "results = db.similarity_search_with_relevance_scores(query_text, k=5)  # Récupérer plus de résultats pour le ranking\n",
    "results.sort(key=lambda x: x[1], reverse=True)  # Tri par score de pertinence\n",
    "top_results = results[:3]  # Prendre les 3 meilleurs résultats\n",
    "\n",
    "context_text = \"\\n\\n - -\\n\\n\".join([doc.page_content for doc, _score in top_results])\n",
    "\n",
    "# Compress the prompt\n",
    "compressed_prompt = compress_prompt(context_text, query_text)\n",
    "\n",
    "# Create prompt using compressed context and query text\n",
    "prompt_template = ChatPromptTemplate.from_template(\"\"\"\n",
    "    Tu t'appelles Okka.\n",
    "    Tu es un assistant en intelligence artificielle conçu pour aider les utilisateurs en récupérant des informations pertinentes et en générant des réponses basées sur ces informations.\n",
    "    Ton objectif est de fournir des réponses claires et utiles.\n",
    "\n",
    "    {compressed_prompt}\n",
    "    \"\"\")\n",
    "\n",
    "# Generate response based on the prompt\n",
    "response_text = model.invoke(prompt_template.format(compressed_prompt=compressed_prompt))\n",
    "\n",
    "# Get sources of the relevant documents\n",
    "sources = [doc.metadata.get(\"source\", None) for doc, _score in top_results]\n",
    "\n",
    "# Format and return the response\n",
    "formatted_response = f\"Response: {response_text}\\nSources: {sources}\"\n",
    "print(formatted_response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envpy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
